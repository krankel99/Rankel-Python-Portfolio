{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Assignment HW 3/31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from spacy) (2.11.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from spacy) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.0)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\kevin\\anaconda3\\envs\\pandas-playground\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 2.1/12.8 MB 13.0 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 3.1/12.8 MB 8.4 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.5/12.8 MB 7.7 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.8/12.8 MB 8.2 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 9.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.5/12.8 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 9.1 MB/s eta 0:00:00\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Installing the spaCy library\n",
    "!pip install spacy\n",
    "\n",
    "# Installing the small English language model used by spaCy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"The quick brown fox doesn't jump over the lazy dog. Natural Language Processing is fascinating!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'does', \"n't\", 'jump', 'over', 'the', 'lazy', 'dog', '.', 'Natural', 'Language', 'Processing', 'is', 'fascinating', '!']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# Load spaCy's small English model\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "# Apply the NLP pipeline to the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract tokens\n",
    "tokens_spacy = [token.text for token in doc]\n",
    "print(tokens_spacy)\n",
    "\n",
    "# for punctuation spaCy just makes that a separate token compared to the split function from class which would just add the punctuation on to the end of the previous token.\n",
    "# interesting to see how how the word \"doesn't\", spacy knows to break it down and it \"does\" and \"n't\", which at first I thought was a typo when I saw it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "quick\n",
      "brown\n",
      "fox\n",
      "does\n",
      "n't\n",
      "jump\n",
      "over\n",
      "the\n",
      "lazy\n",
      "dog\n",
      ".\n",
      "Natural\n",
      "Language\n",
      "Processing\n",
      "is\n",
      "fascinating\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)\n",
    "# This seems to be the same way as it tokenized it above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fox\n",
      "fox\n",
      "fox\n",
      "jump\n",
      "jump\n",
      "jump\n",
      "jump\n",
      "jump\n",
      "dog\n",
      "dog\n",
      "over\n",
      "jump\n",
      "Language\n",
      "Processing\n",
      "is\n",
      "is\n",
      "is\n",
      "is\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.head.text)\n",
    "# now this shocked me at first, but after reading more about spaCy it is just telling us which word it is governed by so like fox, does, n't, jump, and over are all governed by jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "quick\n",
      "brown\n",
      "fox\n",
      "do\n",
      "not\n",
      "jump\n",
      "over\n",
      "the\n",
      "lazy\n",
      "dog\n",
      ".\n",
      "Natural\n",
      "Language\n",
      "processing\n",
      "be\n",
      "fascinating\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.lemma_)\n",
    "# this just contains no inflectional suffixes, as you can see that does turned to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definite=Def|PronType=Art\n",
      "Degree=Pos\n",
      "Degree=Pos\n",
      "Number=Sing\n",
      "Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "Polarity=Neg\n",
      "VerbForm=Inf\n",
      "\n",
      "Definite=Def|PronType=Art\n",
      "Degree=Pos\n",
      "Number=Sing\n",
      "PunctType=Peri\n",
      "Number=Sing\n",
      "Number=Sing\n",
      "Number=Sing\n",
      "Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "Degree=Pos\n",
      "PunctType=Peri\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.morph)\n",
    "# so this one is a little more confusing at first, but from what I understand:\n",
    "# Definite=Def|PronType=Art is for a definite article vs a normal article respectively\n",
    "# Degree=Pos is for the positive degree, like the base form of an adjective\n",
    "# Number=Sing is saying the word is singular\n",
    "# Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin is for verbs and describing the mood, the subject, third person, present tense and it is a finite verb\n",
    "# Polarity=Neg is for negation\n",
    "# VerbForm=Inf is for infinitive verb form which is common after auxiliary verbs\n",
    "# PunctType=Peri is for the period like punctuation but it is interesting that there is only one of those"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "84\n",
      "84\n",
      "92\n",
      "87\n",
      "94\n",
      "100\n",
      "85\n",
      "90\n",
      "84\n",
      "92\n",
      "97\n",
      "96\n",
      "96\n",
      "92\n",
      "87\n",
      "84\n",
      "97\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.pos)\n",
    "# quick is 84, jump is 100, is is 87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15267657372422890137\n",
      "10554686591937588953\n",
      "10554686591937588953\n",
      "15308085513773655218\n",
      "13927759927860985106\n",
      "164681854541413346\n",
      "14200088355797579614\n",
      "1292078113972184607\n",
      "15267657372422890137\n",
      "10554686591937588953\n",
      "15308085513773655218\n",
      "12646065887601541794\n",
      "15794550382381185553\n",
      "15794550382381185553\n",
      "15308085513773655218\n",
      "13927759927860985106\n",
      "10554686591937588953\n",
      "12646065887601541794\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.tag)\n",
    "# quick is 10554686591937588953\n",
    "# jump is 14200088355797579614\n",
    "# is is 13927759927860985106\n",
    "\n",
    "# It is important for grammar checking because it makes it easier to machines to see which word doesn't belong like if it should be a verb or a noun\n",
    "# Machine translation benefits from this as well similarily to grammer checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2=\"Barack Obama was the 44th President of the United States. He was born in Hawaii.\"\n",
    "doc1=nlp(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Obama PERSON\n",
      "44th ORDINAL\n",
      "the United States GPE\n",
      "Hawaii GPE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc1.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "# Breaks down by showing that Barack Obama is a Person\n",
    "# the 44th is ordinal because it just that Barack was the 44th President which is an order\n",
    "# the US and Hawaii are both geopolitical entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack PROPN\n",
      "Obama PROPN\n",
      "was AUX\n",
      "the DET\n",
      "44th ADJ\n",
      "President PROPN\n",
      "of ADP\n",
      "the DET\n",
      "United PROPN\n",
      "States PROPN\n",
      ". PUNCT\n",
      "He PRON\n",
      "was AUX\n",
      "born VERB\n",
      "in ADP\n",
      "Hawaii PROPN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(token.text, token.pos_)\n",
    "# here is a little less specific showing that both Barack and Obama are Proper Nouns as are United States and Hawaii where they were broken up above\n",
    "# 44th is also just an adjective here and not ordinal\n",
    "# punctuation is clearly labeled though as punctuation and it also breaks down each word unlike above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sentence= \"I and Daniel don't like too play socer together in the backyard.. We pratcice all the time!\"\n",
    "doc2=nlp(my_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daniel PERSON\n"
     ]
    }
   ],
   "source": [
    "for ent in doc2.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "# only recognizes Daniel as a person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON I PRP\n",
      "and CCONJ and CC\n",
      "Daniel PROPN Daniel NNP\n",
      "do AUX do VBP\n",
      "n't PART not RB\n",
      "like VERB like VB\n",
      "too ADV too RB\n",
      "play VERB play VB\n",
      "socer NOUN socer NN\n",
      "together ADV together RB\n",
      "in ADP in IN\n",
      "the DET the DT\n",
      "backyard NOUN backyard NN\n",
      ".. PUNCT .. .\n",
      "We PRON we PRP\n",
      "pratcice VERB pratcice VBP\n",
      "all DET all PDT\n",
      "the DET the DT\n",
      "time NOUN time NN\n",
      "! PUNCT ! .\n"
     ]
    }
   ],
   "source": [
    "for token in doc2:\n",
    "    print(token.text, token.pos_, token.lemma_, token.tag_)\n",
    "# important to notice how \"socer\" isn't corrected, but it still knows that it is a noun. Similar to how it knows pratcice is a verb.\n",
    "# the double period is just punctuation as well\n",
    "# the entities and tags do not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like\n",
      "I\n",
      "I\n",
      "like\n",
      "like\n",
      "like\n",
      "play\n",
      "socer\n",
      "like\n",
      "like\n",
      "like\n",
      "backyard\n",
      "in\n",
      "like\n",
      "pratcice\n",
      "pratcice\n",
      "time\n",
      "time\n",
      "pratcice\n",
      "pratcice\n"
     ]
    }
   ],
   "source": [
    "for token in doc2:\n",
    "    print(token.head.text)\n",
    "# I threw this in because I thought it was really cool and unlike the previous sentence that I put through the head.text, this one kept jumping back and forth between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pandas-Playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
